{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace with the actual path to your file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/ibrah/OneDrive/Masaüstü/all_companies_balance_statements.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace with the actual path to your file\n",
    "df = pd.read_csv(\"C:/Users/ibrah/OneDrive/Masaüstü/all_companies_balance_statements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the actual path to your file\n",
    "df2 = pd.read_csv(\"C:/Users/ibrah/OneDrive/Masaüstü/all_companies_cashflow_statements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the actual path to your file\n",
    "df3 = pd.read_csv(\"C:/Users/ibrah/OneDrive/Masaüstü/all_companies_income_statements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the actual path to your file\n",
    "df4 = pd.read_excel(\"C:/Users/ibrah/OneDrive/Masaüstü/Nimbus-Fintegral Strateji/listof90year.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# First, rename columns in each DataFrame to avoid collisions (except the join keys)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_bs \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_cf \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m df_is \u001b[38;5;241m=\u001b[39m df3\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# First, rename columns in each DataFrame to avoid collisions (except the join keys)\n",
    "df_bs = df.add_prefix('bs_')\n",
    "df_cf = df2.add_prefix('cf_')\n",
    "df_is = df3.add_prefix('is_')\n",
    "\n",
    "# But restore the join keys to their original names\n",
    "for col in ['symbol', 'calendarYear', 'period']:\n",
    "    df_bs[col] = df[col]\n",
    "    df_cf[col] = df2[col]\n",
    "    df_is[col] = df3[col]\n",
    "\n",
    "# Now merge them one by one on 'symbol', 'calendarYear', and 'period'\n",
    "combined_df = df_bs.merge(df_cf, on=['symbol', 'calendarYear', 'period'], how='outer')\n",
    "combined_df = combined_df.merge(df_is, on=['symbol', 'calendarYear', 'period'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "combined_df.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"C:/Users/ibrah/OneDrive/Masaüstü/newformat.xlsx\"\n",
    "\n",
    "combined_df.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross Profit / Total Assets\n",
    "combined_df['gross_profit'] = combined_df['is_revenue'] - combined_df['is_costOfRevenue']\n",
    "combined_df['gross_ROA'] = combined_df['gross_profit'] / combined_df['bs_totalCurrentAssets']\n",
    "\n",
    "# EBIT / Market Price (using total stockholders equity as market price proxy)\n",
    "combined_df['EBIT_to_MP'] = combined_df['is_ebitda'] / combined_df['bs_totalStockholdersEquity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['gross_ROA'] = combined_df['gross_ROA'].replace([np.inf, -np.inf], np.nan)\n",
    "combined_df['EBIT_to_MP'] = combined_df['EBIT_to_MP'].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a list to collect results for each year\n",
    "ranked_results = []\n",
    "\n",
    "# Loop over each row in df4 (each year)\n",
    "for idx, row in df4.iterrows():\n",
    "    year = row['year']\n",
    "    prev_year = year - 1\n",
    "\n",
    "    # Get the tickers for this year, drop NaNs and flatten\n",
    "    tickers = row[1:].dropna().unique().tolist()\n",
    "\n",
    "    # Filter combined_df for previous year's financials and relevant tickers\n",
    "    df_year = combined_df[\n",
    "        (combined_df['calendarYear'] == prev_year) &\n",
    "        (combined_df['symbol'].isin(tickers))\n",
    "    ].copy()\n",
    "\n",
    "    # Only proceed if there's data\n",
    "    if df_year.empty:\n",
    "        continue\n",
    "\n",
    "    # Rank by gross_ROA (higher is better)\n",
    "    df_year['rank_gross_ROA'] = df_year['gross_ROA'].rank(ascending=False, method='min')\n",
    "\n",
    "    # Rank by EBIT_to_MP (higher is better)\n",
    "    df_year['rank_EBIT_to_MP'] = df_year['EBIT_to_MP'].rank(ascending=False, method='min')\n",
    "\n",
    "    # Sum ranks\n",
    "    df_year['combined_score'] = df_year['rank_gross_ROA'] + df_year['rank_EBIT_to_MP']\n",
    "\n",
    "    # Final ranking based on combined score (lower is better)\n",
    "    df_year['final_rank'] = df_year['combined_score'].rank(method='min')\n",
    "\n",
    "    # Add current year for context\n",
    "    df_year['ranking_year'] = year\n",
    "\n",
    "    # Save result\n",
    "    ranked_results.append(df_year)\n",
    "\n",
    "# Concatenate all results into a single DataFrame\n",
    "ranked_df = pd.concat(ranked_results, ignore_index=True)\n",
    "\n",
    "# Optional: sort by year and final rank\n",
    "ranked_df = ranked_df.sort_values(by=['ranking_year', 'final_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bs_Symbol</th>\n",
       "      <th>bs_Unnamed: 1</th>\n",
       "      <th>bs_date</th>\n",
       "      <th>bs_symbol</th>\n",
       "      <th>bs_reportedCurrency</th>\n",
       "      <th>bs_cik</th>\n",
       "      <th>bs_fillingDate</th>\n",
       "      <th>bs_acceptedDate</th>\n",
       "      <th>bs_calendarYear</th>\n",
       "      <th>bs_period</th>\n",
       "      <th>...</th>\n",
       "      <th>is_link</th>\n",
       "      <th>is_finalLink</th>\n",
       "      <th>gross_profit</th>\n",
       "      <th>gross_ROA</th>\n",
       "      <th>EBIT_to_MP</th>\n",
       "      <th>rank_gross_ROA</th>\n",
       "      <th>rank_EBIT_to_MP</th>\n",
       "      <th>combined_score</th>\n",
       "      <th>final_rank</th>\n",
       "      <th>ranking_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>GIS</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-05-30</td>\n",
       "      <td>GIS</td>\n",
       "      <td>USD</td>\n",
       "      <td>40704.0</td>\n",
       "      <td>1999-08-23</td>\n",
       "      <td>1999-08-23 00:00:00</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>FY</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/40704/...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/40704/...</td>\n",
       "      <td>3.846800e+09</td>\n",
       "      <td>3.489161</td>\n",
       "      <td>7.380633</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>CPB</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-08-01</td>\n",
       "      <td>CPB</td>\n",
       "      <td>USD</td>\n",
       "      <td>16732.0</td>\n",
       "      <td>1999-10-12</td>\n",
       "      <td>1999-10-12 00:00:00</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>FY</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/16732/...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/16732/...</td>\n",
       "      <td>3.571000e+09</td>\n",
       "      <td>2.759660</td>\n",
       "      <td>6.668085</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>K</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>K</td>\n",
       "      <td>USD</td>\n",
       "      <td>55067.0</td>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>2000-03-24 00:00:00</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>FY</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/55067/...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/55067/...</td>\n",
       "      <td>3.947100e+09</td>\n",
       "      <td>2.515358</td>\n",
       "      <td>1.674127</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>EXC</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>EXC</td>\n",
       "      <td>USD</td>\n",
       "      <td>1109357.0</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>1999-12-30 19:00:00</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>FY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.291600e+09</td>\n",
       "      <td>2.714498</td>\n",
       "      <td>0.927611</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PEP</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-12-25</td>\n",
       "      <td>PEP</td>\n",
       "      <td>USD</td>\n",
       "      <td>77476.0</td>\n",
       "      <td>2000-03-21</td>\n",
       "      <td>2000-03-21 00:00:00</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>FY</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/77476/...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/77476/...</td>\n",
       "      <td>1.301800e+10</td>\n",
       "      <td>3.119578</td>\n",
       "      <td>0.684784</td>\n",
       "      <td>20.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9880</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/113686...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/113686...</td>\n",
       "      <td>1.278900e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/91576/...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/91576/...</td>\n",
       "      <td>8.427000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/746515...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/746515...</td>\n",
       "      <td>1.060052e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/879101...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/879101...</td>\n",
       "      <td>1.399361e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/107075...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/107075...</td>\n",
       "      <td>5.684000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9885 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bs_Symbol  bs_Unnamed: 1     bs_date bs_symbol bs_reportedCurrency  \\\n",
       "137        GIS           25.0  1999-05-30       GIS                 USD   \n",
       "196        CPB           25.0  1999-08-01       CPB                 USD   \n",
       "147          K           24.0  1999-12-31         K                 USD   \n",
       "125        EXC           25.0  1999-12-31       EXC                 USD   \n",
       "27         PEP           25.0  1999-12-25       PEP                 USD   \n",
       "...        ...            ...         ...       ...                 ...   \n",
       "9880       NaN            NaN         NaN       NaN                 NaN   \n",
       "9881       NaN            NaN         NaN       NaN                 NaN   \n",
       "9882       NaN            NaN         NaN       NaN                 NaN   \n",
       "9883       NaN            NaN         NaN       NaN                 NaN   \n",
       "9884       NaN            NaN         NaN       NaN                 NaN   \n",
       "\n",
       "         bs_cik bs_fillingDate      bs_acceptedDate  bs_calendarYear  \\\n",
       "137     40704.0     1999-08-23  1999-08-23 00:00:00           1999.0   \n",
       "196     16732.0     1999-10-12  1999-10-12 00:00:00           1999.0   \n",
       "147     55067.0     2000-03-24  2000-03-24 00:00:00           1999.0   \n",
       "125   1109357.0     1999-12-31  1999-12-30 19:00:00           1999.0   \n",
       "27      77476.0     2000-03-21  2000-03-21 00:00:00           1999.0   \n",
       "...         ...            ...                  ...              ...   \n",
       "9880        NaN            NaN                  NaN              NaN   \n",
       "9881        NaN            NaN                  NaN              NaN   \n",
       "9882        NaN            NaN                  NaN              NaN   \n",
       "9883        NaN            NaN                  NaN              NaN   \n",
       "9884        NaN            NaN                  NaN              NaN   \n",
       "\n",
       "     bs_period  ...                                            is_link  \\\n",
       "137         FY  ...  https://www.sec.gov/Archives/edgar/data/40704/...   \n",
       "196         FY  ...  https://www.sec.gov/Archives/edgar/data/16732/...   \n",
       "147         FY  ...  https://www.sec.gov/Archives/edgar/data/55067/...   \n",
       "125         FY  ...                                                NaN   \n",
       "27          FY  ...  https://www.sec.gov/Archives/edgar/data/77476/...   \n",
       "...        ...  ...                                                ...   \n",
       "9880       NaN  ...  https://www.sec.gov/Archives/edgar/data/113686...   \n",
       "9881       NaN  ...  https://www.sec.gov/Archives/edgar/data/91576/...   \n",
       "9882       NaN  ...  https://www.sec.gov/Archives/edgar/data/746515...   \n",
       "9883       NaN  ...  https://www.sec.gov/Archives/edgar/data/879101...   \n",
       "9884       NaN  ...  https://www.sec.gov/Archives/edgar/data/107075...   \n",
       "\n",
       "                                           is_finalLink  gross_profit  \\\n",
       "137   https://www.sec.gov/Archives/edgar/data/40704/...  3.846800e+09   \n",
       "196   https://www.sec.gov/Archives/edgar/data/16732/...  3.571000e+09   \n",
       "147   https://www.sec.gov/Archives/edgar/data/55067/...  3.947100e+09   \n",
       "125                                                 NaN  3.291600e+09   \n",
       "27    https://www.sec.gov/Archives/edgar/data/77476/...  1.301800e+10   \n",
       "...                                                 ...           ...   \n",
       "9880  https://www.sec.gov/Archives/edgar/data/113686...  1.278900e+09   \n",
       "9881  https://www.sec.gov/Archives/edgar/data/91576/...  8.427000e+09   \n",
       "9882  https://www.sec.gov/Archives/edgar/data/746515...  1.060052e+10   \n",
       "9883  https://www.sec.gov/Archives/edgar/data/879101...  1.399361e+09   \n",
       "9884  https://www.sec.gov/Archives/edgar/data/107075...  5.684000e+09   \n",
       "\n",
       "      gross_ROA  EBIT_to_MP  rank_gross_ROA  rank_EBIT_to_MP  combined_score  \\\n",
       "137    3.489161    7.380633            15.0              3.0            18.0   \n",
       "196    2.759660    6.668085            25.0              5.0            30.0   \n",
       "147    2.515358    1.674127            33.0              9.0            42.0   \n",
       "125    2.714498    0.927611            27.0             16.0            43.0   \n",
       "27     3.119578    0.684784            20.0             31.0            51.0   \n",
       "...         ...         ...             ...              ...             ...   \n",
       "9880        NaN         NaN             NaN              NaN             NaN   \n",
       "9881        NaN         NaN             NaN              NaN             NaN   \n",
       "9882        NaN         NaN             NaN              NaN             NaN   \n",
       "9883        NaN         NaN             NaN              NaN             NaN   \n",
       "9884        NaN         NaN             NaN              NaN             NaN   \n",
       "\n",
       "      final_rank  ranking_year  \n",
       "137          1.0          2000  \n",
       "196          2.0          2000  \n",
       "147          3.0          2000  \n",
       "125          4.0          2000  \n",
       "27           5.0          2000  \n",
       "...          ...           ...  \n",
       "9880         NaN          2025  \n",
       "9881         NaN          2025  \n",
       "9882         NaN          2025  \n",
       "9883         NaN          2025  \n",
       "9884         NaN          2025  \n",
       "\n",
       "[9885 rows x 149 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"C:/Users/ibrah/OneDrive/Masaüstü/ranking.xlsx\"\n",
    "\n",
    "ranked_df.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "1 Failed download:\n",
      "['HCA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2000-04-01 -> 2001-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 954565200, endDate = 985928400\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "2 Failed downloads:\n",
      "['HCA', 'PLL']: YFPricesMissingError('possibly delisted; no price data found  (1d 2001-04-01 -> 2002-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 986101200, endDate = 1017464400\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "1 Failed download:\n",
      "['HCA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-01 -> 2003-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 1017637200, endDate = 1049000400\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "1 Failed download:\n",
      "['HCA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2003-04-01 -> 2004-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 1049173200, endDate = 1080622800\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "1 Failed download:\n",
      "['HCA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2004-04-01 -> 2005-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 1080795600, endDate = 1112158800\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "2 Failed downloads:\n",
      "['HCA', 'PLL']: YFPricesMissingError('possibly delisted; no price data found  (1d 2005-04-01 -> 2006-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 1112331600, endDate = 1143694800\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "1 Failed download:\n",
      "['HCA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2006-04-01 -> 2007-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 1143867600, endDate = 1175227200\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "\n",
      "1 Failed download:\n",
      "['MMI']: YFPricesMissingError('possibly delisted; no price data found  (1d 2012-04-01 -> 2013-03-30) (Yahoo error = \"Data doesn\\'t exist for startDate = 1333252800, endDate = 1364616000\")')\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n",
      "[*********************100%***********************]  20 of 20 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "portfolio_returns = []\n",
    "\n",
    "for year in range(2000, 2025):\n",
    "    buy_date = f\"{year}-04-01\"\n",
    "    sell_date = f\"{year+1}-03-30\"\n",
    "    \n",
    "    top20 = ranked_df[ranked_df['ranking_year'] == year].nsmallest(20, 'final_rank')\n",
    "    tickers = top20['symbol'].dropna().unique().tolist()\n",
    "\n",
    "    if len(tickers) == 0:\n",
    "        continue\n",
    "\n",
    "    # Download adjusted close prices\n",
    "    price_data = yf.download(tickers, start=buy_date, end=sell_date)['Close']\n",
    "\n",
    "    # Ensure price_data is a DataFrame\n",
    "    if isinstance(price_data, pd.Series):\n",
    "        price_data = price_data.to_frame()\n",
    "\n",
    "    # Forward-fill to handle missing weekends/holidays\n",
    "    price_data = price_data.ffill().bfill()\n",
    "\n",
    "    # Convert buy/sell dates to Timestamps and find closest available dates\n",
    "    buy_date_actual = price_data.index[price_data.index.get_indexer([pd.Timestamp(buy_date)], method='nearest')[0]]\n",
    "    sell_date_actual = price_data.index[price_data.index.get_indexer([pd.Timestamp(sell_date)], method='nearest')[0]]\n",
    "\n",
    "    # Extract prices on closest available dates\n",
    "    buy_prices = price_data.loc[buy_date_actual]\n",
    "    sell_prices = price_data.loc[sell_date_actual]\n",
    "\n",
    "    # Calculate returns\n",
    "    returns = (sell_prices / buy_prices) - 1\n",
    "    avg_return = returns.mean()\n",
    "\n",
    "    portfolio_returns.append({\n",
    "        'year': year,\n",
    "        'buy_date': str(buy_date_actual.date()),\n",
    "        'sell_date': str(sell_date_actual.date()),\n",
    "        'average_return': avg_return,\n",
    "        'top20_tickers': tickers\n",
    "    })\n",
    "\n",
    "# Create final results DataFrame\n",
    "results_df = pd.DataFrame(portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>buy_date</th>\n",
       "      <th>sell_date</th>\n",
       "      <th>average_return</th>\n",
       "      <th>top20_tickers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000-04-03</td>\n",
       "      <td>2001-03-29</td>\n",
       "      <td>0.374879</td>\n",
       "      <td>[GIS, CPB, K, EXC, PEP, CL, HCA, KR, SLM, TROW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>2002-03-28</td>\n",
       "      <td>0.233820</td>\n",
       "      <td>[PLL, DLX, CPB, EHC, COP, EOG, KR, K, CL, NYT,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>2003-03-28</td>\n",
       "      <td>-0.072632</td>\n",
       "      <td>[YUM, DLX, GIS, HCA, EOG, SLM, K, CL, KR, MO, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04-01</td>\n",
       "      <td>2004-03-29</td>\n",
       "      <td>0.421360</td>\n",
       "      <td>[YUM, DLX, CL, EFX, KR, K, CNP, TROW, HCA, VZ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>2004-04-01</td>\n",
       "      <td>2005-03-29</td>\n",
       "      <td>0.185571</td>\n",
       "      <td>[YUM, HCA, CPB, CL, EFX, SPG, K, VZ, EOG, KR, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005-04-01</td>\n",
       "      <td>2006-03-29</td>\n",
       "      <td>0.178355</td>\n",
       "      <td>[PLL, YUM, HCA, SPG, FNMA, CL, EFX, CPB, PGR, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>2006-04-03</td>\n",
       "      <td>2007-03-29</td>\n",
       "      <td>0.204547</td>\n",
       "      <td>[HCA, CL, YUM, FNMA, R, KR, EFX, VZ, K, CPB, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>2007-04-02</td>\n",
       "      <td>2008-03-28</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>[YUM, R, CL, KR, DRI, EFX, UPS, K, VZ, XOM, MC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>2008-04-01</td>\n",
       "      <td>2009-03-27</td>\n",
       "      <td>-0.296433</td>\n",
       "      <td>[R, CLX, CL, EL, RSG, KR, CPB, DRI, VZ, EXC, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2010-03-29</td>\n",
       "      <td>0.527616</td>\n",
       "      <td>[UPS, R, CL, XOM, K, KR, DRI, CPB, SPG, PEP, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>2011-03-29</td>\n",
       "      <td>0.255093</td>\n",
       "      <td>[R, VZ, YUM, UPS, CL, K, CPB, DRI, MCD, EXC, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2012-03-29</td>\n",
       "      <td>0.148974</td>\n",
       "      <td>[R, UPS, CLX, CL, VZ, KR, SPG, TPR, CPB, MCD, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012-04-02</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>0.175034</td>\n",
       "      <td>[MMI, R, UPS, CL, TRIP, KR, MCD, TPR, VZ, KMI,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>2014-03-28</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>[THC, VZ, CL, KR, AIV, YUM, CPB, KDP, MCD, DRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>2015-03-27</td>\n",
       "      <td>0.217838</td>\n",
       "      <td>[PRU, THC, CL, UPS, YUM, CLX, KR, DLTR, AIV, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2016-03-29</td>\n",
       "      <td>0.062503</td>\n",
       "      <td>[PRU, VRSK, AIZ, THC, VZ, UPS, CL, YUM, KR, SH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>[VZ, UNM, THC, PRU, AIZ, UPS, SHW, SPG, YUM, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>0.124829</td>\n",
       "      <td>[PRU, UNM, VZ, SPG, WU, INTU, AIZ, HD, KR, CLX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>0.177905</td>\n",
       "      <td>[UNM, PRU, INTU, FE, VZ, HD, SPG, CLX, KDP, AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>-0.149972</td>\n",
       "      <td>[PRU, SPG, CLX, KR, MO, VZ, AMT, URI, JWN, AIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>1.149953</td>\n",
       "      <td>[SPG, AIV, CL, MO, IDXX, CLX, URI, AMT, PEP, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>0.142307</td>\n",
       "      <td>[HCA, CL, MO, MTD, HD, CHTR, AMT, KR, IRM, SHW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>-0.036820</td>\n",
       "      <td>[CL, CHTR, SPG, IDXX, IRM, MTD, UNP, CLX, AMT,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023</td>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>2024-03-28</td>\n",
       "      <td>0.218870</td>\n",
       "      <td>[PRU, CHTR, APA, SPG, UNP, CL, IRM, MTD, CZR, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2024</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>0.146680</td>\n",
       "      <td>[CHTR, NCLH, VRSK, GDDY, CL, APA, RCL, IRM, AM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year    buy_date   sell_date  average_return  \\\n",
       "0   2000  2000-04-03  2001-03-29        0.374879   \n",
       "1   2001  2001-04-02  2002-03-28        0.233820   \n",
       "2   2002  2002-04-01  2003-03-28       -0.072632   \n",
       "3   2003  2003-04-01  2004-03-29        0.421360   \n",
       "4   2004  2004-04-01  2005-03-29        0.185571   \n",
       "5   2005  2005-04-01  2006-03-29        0.178355   \n",
       "6   2006  2006-04-03  2007-03-29        0.204547   \n",
       "7   2007  2007-04-02  2008-03-28        0.001782   \n",
       "8   2008  2008-04-01  2009-03-27       -0.296433   \n",
       "9   2009  2009-04-01  2010-03-29        0.527616   \n",
       "10  2010  2010-04-01  2011-03-29        0.255093   \n",
       "11  2011  2011-04-01  2012-03-29        0.148974   \n",
       "12  2012  2012-04-02  2013-03-28        0.175034   \n",
       "13  2013  2013-04-01  2014-03-28        0.060741   \n",
       "14  2014  2014-04-01  2015-03-27        0.217838   \n",
       "15  2015  2015-04-01  2016-03-29        0.062503   \n",
       "16  2016  2016-04-01  2017-03-29        0.178922   \n",
       "17  2017  2017-04-03  2018-03-29        0.124829   \n",
       "18  2018  2018-04-02  2019-03-29        0.177905   \n",
       "19  2019  2019-04-01  2020-03-27       -0.149972   \n",
       "20  2020  2020-04-01  2021-03-29        1.149953   \n",
       "21  2021  2021-04-01  2022-03-29        0.142307   \n",
       "22  2022  2022-04-01  2023-03-29       -0.036820   \n",
       "23  2023  2023-04-03  2024-03-28        0.218870   \n",
       "24  2024  2024-04-01  2025-03-24        0.146680   \n",
       "\n",
       "                                        top20_tickers  \n",
       "0   [GIS, CPB, K, EXC, PEP, CL, HCA, KR, SLM, TROW...  \n",
       "1   [PLL, DLX, CPB, EHC, COP, EOG, KR, K, CL, NYT,...  \n",
       "2   [YUM, DLX, GIS, HCA, EOG, SLM, K, CL, KR, MO, ...  \n",
       "3   [YUM, DLX, CL, EFX, KR, K, CNP, TROW, HCA, VZ,...  \n",
       "4   [YUM, HCA, CPB, CL, EFX, SPG, K, VZ, EOG, KR, ...  \n",
       "5   [PLL, YUM, HCA, SPG, FNMA, CL, EFX, CPB, PGR, ...  \n",
       "6   [HCA, CL, YUM, FNMA, R, KR, EFX, VZ, K, CPB, A...  \n",
       "7   [YUM, R, CL, KR, DRI, EFX, UPS, K, VZ, XOM, MC...  \n",
       "8   [R, CLX, CL, EL, RSG, KR, CPB, DRI, VZ, EXC, S...  \n",
       "9   [UPS, R, CL, XOM, K, KR, DRI, CPB, SPG, PEP, E...  \n",
       "10  [R, VZ, YUM, UPS, CL, K, CPB, DRI, MCD, EXC, T...  \n",
       "11  [R, UPS, CLX, CL, VZ, KR, SPG, TPR, CPB, MCD, ...  \n",
       "12  [MMI, R, UPS, CL, TRIP, KR, MCD, TPR, VZ, KMI,...  \n",
       "13  [THC, VZ, CL, KR, AIV, YUM, CPB, KDP, MCD, DRI...  \n",
       "14  [PRU, THC, CL, UPS, YUM, CLX, KR, DLTR, AIV, M...  \n",
       "15  [PRU, VRSK, AIZ, THC, VZ, UPS, CL, YUM, KR, SH...  \n",
       "16  [VZ, UNM, THC, PRU, AIZ, UPS, SHW, SPG, YUM, C...  \n",
       "17  [PRU, UNM, VZ, SPG, WU, INTU, AIZ, HD, KR, CLX...  \n",
       "18  [UNM, PRU, INTU, FE, VZ, HD, SPG, CLX, KDP, AI...  \n",
       "19  [PRU, SPG, CLX, KR, MO, VZ, AMT, URI, JWN, AIZ...  \n",
       "20  [SPG, AIV, CL, MO, IDXX, CLX, URI, AMT, PEP, J...  \n",
       "21  [HCA, CL, MO, MTD, HD, CHTR, AMT, KR, IRM, SHW...  \n",
       "22  [CL, CHTR, SPG, IDXX, IRM, MTD, UNP, CLX, AMT,...  \n",
       "23  [PRU, CHTR, APA, SPG, UNP, CL, IRM, MTD, CZR, ...  \n",
       "24  [CHTR, NCLH, VRSK, GDDY, CL, APA, RCL, IRM, AM...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Scrape the stock transactions from Senator periodic filings for the last week.\n",
    "Not: Bu kod, https://efdsearch.senate.gov üzerinden verileri çekmektedir.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "from typing import List, Optional, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL tanımlamaları\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "LANDING_PAGE_FAIL = 'Failed to fetch filings landing page'\n",
    "\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',\n",
    "    'file_date',\n",
    "    'last_name',\n",
    "    'first_name',\n",
    "    'order_type',\n",
    "    'ticker',\n",
    "    'asset_name',\n",
    "    'tx_amount'\n",
    "]\n",
    "\n",
    "# Loglama ayarları\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def add_rate_limit(func):\n",
    "    \"\"\"Decorator: Her çağrıdan önce belirli bir süre bekler.\"\"\"\n",
    "    def with_rate_limit(*args, **kwargs):\n",
    "        time.sleep(RATE_LIMIT_SECS)\n",
    "        return func(*args, **kwargs)\n",
    "    return with_rate_limit\n",
    "\n",
    "\n",
    "def _csrf(client: requests.Session) -> str:\n",
    "    \"\"\"\n",
    "    Oturum için CSRF token'ını alır.\n",
    "    Landing page’i ziyaret edip, form üzerinden token'ı çekip gönderir.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        landing_page_response = client.get(LANDING_PAGE_URL)\n",
    "    except Exception as e:\n",
    "        LOGGER.error(f\"Landing page alınırken hata: {e}\")\n",
    "        raise\n",
    "\n",
    "    if landing_page_response.url != LANDING_PAGE_URL:\n",
    "        raise Exception(LANDING_PAGE_FAIL)\n",
    "\n",
    "    soup = BeautifulSoup(landing_page_response.text, 'lxml')\n",
    "    csrf_tag = soup.find(attrs={'name': 'csrfmiddlewaretoken'})\n",
    "    if not csrf_tag:\n",
    "        raise ValueError(\"CSRF token bulunamadı.\")\n",
    "    form_csrf = csrf_tag.get('value')\n",
    "    payload = {\n",
    "        'csrfmiddlewaretoken': form_csrf,\n",
    "        'prohibition_agreement': '1'\n",
    "    }\n",
    "    client.post(LANDING_PAGE_URL, data=payload, headers={'Referer': LANDING_PAGE_URL})\n",
    "\n",
    "    csrftoken = client.cookies.get('csrftoken', client.cookies.get('csrf'))\n",
    "    if not csrftoken:\n",
    "        raise ValueError(\"CSRF token cookie'leri bulunamadı.\")\n",
    "    return csrftoken\n",
    "\n",
    "\n",
    "def reports_api(client: requests.Session, offset: int, token: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Periodic transaction reports API'sini sorgular.\n",
    "    Sadece son 1 haftaya ait verileri çeker.\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    last_week = now - timedelta(days=7)\n",
    "    payload = {\n",
    "        'start': str(offset),\n",
    "        'length': str(BATCH_SIZE),\n",
    "        'report_types': '[11]',\n",
    "        'filer_types': '[]',\n",
    "        'submitted_start_date': last_week.strftime('%m/%d/%Y 00:00:00'),\n",
    "        'submitted_end_date': now.strftime('%m/%d/%Y %H:%M:%S'),\n",
    "        'candidate_state': '',\n",
    "        'senator_state': '',\n",
    "        'office_id': '',\n",
    "        'first_name': '',\n",
    "        'last_name': '',\n",
    "        'csrfmiddlewaretoken': token\n",
    "    }\n",
    "    LOGGER.info(f'Getting rows starting at offset {offset}')\n",
    "    response = client.post(REPORTS_URL, data=payload, headers={'Referer': SEARCH_PAGE_URL})\n",
    "    response.raise_for_status()\n",
    "    json_data = response.json()\n",
    "    data = json_data.get('data', [])\n",
    "    return data\n",
    "\n",
    "\n",
    "def senator_reports(client: requests.Session) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tüm rapor sonuçlarını döner.\n",
    "    Sadece son 1 haftaya ait raporlar çekilir.\n",
    "    \"\"\"\n",
    "    token = _csrf(client)\n",
    "    offset = 0\n",
    "    reports = reports_api(client, offset, token)\n",
    "    all_reports = []\n",
    "    while reports:\n",
    "        all_reports.extend(reports)\n",
    "        offset += BATCH_SIZE\n",
    "        reports = reports_api(client, offset, token)\n",
    "    LOGGER.info(f\"Toplam {len(all_reports)} rapor çekildi.\")\n",
    "    return all_reports\n",
    "\n",
    "\n",
    "def _tbody_from_link(client: requests.Session, link: str) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Belirtilen linkteki sayfadan tbody elementini döner.\n",
    "    Eğer bulunamazsa None döner.\n",
    "    \"\"\"\n",
    "    url = f'{ROOT}{link}'\n",
    "    response = client.get(url)\n",
    "    if response.url == LANDING_PAGE_URL:\n",
    "        LOGGER.info(\"Oturum süresi doldu. CSRF token yenileniyor.\")\n",
    "        _csrf(client)\n",
    "        response = client.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    tbodies = soup.find_all('tbody')\n",
    "    return tbodies[0] if tbodies else None\n",
    "\n",
    "\n",
    "def txs_for_report(client: requests.Session, row: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bir rapor satırından hisse alım-satım işlemleri DataFrame'i oluşturur.\n",
    "    \"\"\"\n",
    "    if len(row) < 5:\n",
    "        LOGGER.warning(f\"Yetersiz sütun içeren satır: {row}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    first, last, _, link_html, date_received = row[:5]\n",
    "    soup_link = BeautifulSoup(link_html, 'lxml')\n",
    "    a_tag = soup_link.find('a')\n",
    "    if not a_tag:\n",
    "        LOGGER.warning(f\"Link bulunamadı in row: {row}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    link = a_tag.get('href')\n",
    "    if link.startswith(PDF_PREFIX):\n",
    "        # PDF formatındaki raporlar işlenemiyor.\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    tbody = _tbody_from_link(client, link)\n",
    "    if not tbody:\n",
    "        LOGGER.warning(f\"tbody bulunamadı: {link}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    transactions = []\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        cols = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "        if len(cols) < 8:\n",
    "            continue\n",
    "        # Beklenen sütun sırası: [?, tx_date, ?, ticker, asset_name, asset_type, order_type, tx_amount, ...]\n",
    "        tx_date = cols[1]\n",
    "        ticker = cols[3]\n",
    "        asset_name = cols[4]\n",
    "        asset_type = cols[5]\n",
    "        order_type = cols[6]\n",
    "        tx_amount = cols[7]\n",
    "        if asset_type != 'Stock' and ticker.strip() in ('--', ''):\n",
    "            continue\n",
    "        transactions.append([tx_date, date_received, last, first, order_type, ticker, asset_name, tx_amount])\n",
    "    \n",
    "    if transactions:\n",
    "        return pd.DataFrame(transactions, columns=REPORT_COL_NAMES)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def main() -> pd.DataFrame:\n",
    "    LOGGER.info(\"Client başlatılıyor.\")\n",
    "    client = requests.Session()\n",
    "    client.get = add_rate_limit(client.get)\n",
    "    client.post = add_rate_limit(client.post)\n",
    "    \n",
    "    reports = senator_reports(client)\n",
    "    txs_frames = []\n",
    "    for idx, row in enumerate(reports):\n",
    "        if idx % 10 == 0:\n",
    "            total = sum(frame.shape[0] for frame in txs_frames)\n",
    "            LOGGER.info(f\"Report #{idx} işleniyor. Şimdiye kadar {total} işlem çekildi.\")\n",
    "        df = txs_for_report(client, row)\n",
    "        if not df.empty:\n",
    "            txs_frames.append(df)\n",
    "    \n",
    "    if txs_frames:\n",
    "        all_txs = pd.concat(txs_frames, ignore_index=True)\n",
    "    else:\n",
    "        all_txs = pd.DataFrame()\n",
    "    \n",
    "    return all_txs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    senator_txs = main()\n",
    "    output_file = 'notebooks/senators.pickle'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(senator_txs, f)\n",
    "    LOGGER.info(f\"Veriler '{output_file}' dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'senator_txs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 12\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43msenator_txs\u001b[49m, f)\n\u001b[1;32m     13\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVeriler \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dosyasına kaydedildi.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'senator_txs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Çıktı dizinini tanımlayın\n",
    "output_dir = 'notebooks'\n",
    "output_file = os.path.join(output_dir, 'senators.pickle')\n",
    "\n",
    "# Eğer dizin mevcut değilse oluşturun\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(senator_txs, f)\n",
    "LOGGER.info(f\"Veriler '{output_file}' dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "from typing import List, Optional, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# URL tanımlamaları\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "LANDING_PAGE_FAIL = 'Failed to fetch filings landing page'\n",
    "\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',\n",
    "    'file_date',\n",
    "    'last_name',\n",
    "    'first_name',\n",
    "    'order_type',\n",
    "    'ticker',\n",
    "    'asset_name',\n",
    "    'tx_amount'\n",
    "]\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Senatör'ün isim soyismi girildiğinde geçmiş işlemlerini gösteren kod\n",
    "Not: Bu kod, https://efdsearch.senate.gov üzerinden verileri çekmektedir.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "from typing import List, Optional, Any\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "LANDING_PAGE_FAIL = 'Failed to fetch filings landing page'\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',     # İşlem tarihi\n",
    "    'file_date',   # Raporun gönderildiği tarih (filing date)\n",
    "    'last_name',   # Senatör soyadı\n",
    "    'first_name',  # Senatör adı\n",
    "    'order_type',  # İşlem türü (alım/satım)\n",
    "    'ticker',      # Hisse sembolü\n",
    "    'asset_name',  # Varlık adı\n",
    "    'tx_amount'    # İşlem tutarı\n",
    "]\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "\n",
    "def add_rate_limit(func):\n",
    "    \"\"\"Her çağrıdan önce belirli bir süre bekleyen decorator.\"\"\"\n",
    "    def with_rate_limit(*args, **kwargs):\n",
    "        time.sleep(RATE_LIMIT_SECS)\n",
    "        return func(*args, **kwargs)\n",
    "    return with_rate_limit\n",
    "def _csrf(client: requests.Session) -> str:\n",
    "    \"\"\"\n",
    "    Oturum için CSRF token'ını alır.\n",
    "    Landing page’i ziyaret edip, form üzerinden token'ı çekip gönderir.\n",
    "    \"\"\"\n",
    "    landing_page_response = client.get(LANDING_PAGE_URL)\n",
    "    if landing_page_response.url != LANDING_PAGE_URL:\n",
    "        raise Exception(LANDING_PAGE_FAIL)\n",
    "\n",
    "    landing_page = BeautifulSoup(landing_page_response.text, 'lxml')\n",
    "    csrf_tag = landing_page.find(attrs={'name': 'csrfmiddlewaretoken'})\n",
    "    if not csrf_tag:\n",
    "        raise ValueError(\"CSRF token bulunamadı.\")\n",
    "    form_csrf = csrf_tag.get('value')\n",
    "    form_payload = {\n",
    "        'csrfmiddlewaretoken': form_csrf,\n",
    "        'prohibition_agreement': '1'\n",
    "    }\n",
    "    client.post(LANDING_PAGE_URL, data=form_payload, headers={'Referer': LANDING_PAGE_URL})\n",
    "\n",
    "    csrftoken = client.cookies.get('csrftoken', client.cookies.get('csrf'))\n",
    "    if not csrftoken:\n",
    "        raise ValueError(\"CSRF token cookie'leri bulunamadı.\")\n",
    "    return csrftoken\n",
    "def reports_api(client: requests.Session, offset: int, token: str,\n",
    "                first_name: str = '', last_name: str = '') -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Periodic transaction reports API'sini sorgular.\n",
    "    Tüm zamanları kapsayacak şekilde 01/01/2012 tarihinden itibaren verileri çeker.\n",
    "    Filtre için isteğe bağlı olarak first_name ve last_name parametreleri verilebilir.\n",
    "    \"\"\"\n",
    "    login_data = {\n",
    "        'start': str(offset),\n",
    "        'length': str(BATCH_SIZE),\n",
    "        'report_types': '[11]',\n",
    "        'filer_types': '[]',\n",
    "        'submitted_start_date': '01/01/2012 00:00:00',\n",
    "        'submitted_end_date': '',  # Bitiş tarihi boş bırakılırsa mevcut tüm veriler alınır.\n",
    "        'candidate_state': '',\n",
    "        'senator_state': '',\n",
    "        'office_id': '',\n",
    "        'first_name': first_name,\n",
    "        'last_name': last_name,\n",
    "        'csrfmiddlewaretoken': token\n",
    "    }\n",
    "    LOGGER.info(f'Getting rows starting at offset {offset}')\n",
    "    response = client.post(REPORTS_URL, data=login_data, headers={'Referer': SEARCH_PAGE_URL})\n",
    "    response.raise_for_status()\n",
    "    return response.json()['data']\n",
    "def senator_reports(client: requests.Session, first_name: str = '', last_name: str = '') -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    İsteğe bağlı olarak belirli bir senatörün (first_name, last_name) verilerini ya da tüm senatörlerin\n",
    "    verilerini döner.\n",
    "    \"\"\"\n",
    "    token = _csrf(client)\n",
    "    idx = 0\n",
    "    reports = reports_api(client, idx, token, first_name, last_name)\n",
    "    all_reports: List[List[str]] = []\n",
    "    while reports:\n",
    "        all_reports.extend(reports)\n",
    "        idx += BATCH_SIZE\n",
    "        reports = reports_api(client, idx, token, first_name, last_name)\n",
    "    LOGGER.info(f\"Toplam {len(all_reports)} rapor çekildi.\")\n",
    "    return all_reports\n",
    "def _tbody_from_link(client: requests.Session, link: str) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Belirtilen linkteki sayfadan tbody elementini döner.\n",
    "    Eğer bulunamazsa None döner.\n",
    "    \"\"\"\n",
    "    report_url = f'{ROOT}{link}'\n",
    "    report_response = client.get(report_url)\n",
    "    if report_response.url == LANDING_PAGE_URL:\n",
    "        LOGGER.info(\"Oturum süresi doldu. CSRF token yenileniyor.\")\n",
    "        _csrf(client)\n",
    "        report_response = client.get(report_url)\n",
    "    report = BeautifulSoup(report_response.text, 'lxml')\n",
    "    tbodies = report.find_all('tbody')\n",
    "    return tbodies[0] if tbodies else None\n",
    "def txs_for_report(client: requests.Session, row: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bir rapor satırından hisse alım-satım işlemleri DataFrame'i oluşturur.\n",
    "    \"\"\"\n",
    "    if len(row) < 5:\n",
    "        LOGGER.warning(f\"Yetersiz sütun içeren satır: {row}\")\n",
    "        return pd.DataFrame()\n",
    "    first, last, _, link_html, date_received = row[:5]\n",
    "    soup_link = BeautifulSoup(link_html, 'lxml')\n",
    "    a_tag = soup_link.find('a')\n",
    "    if not a_tag:\n",
    "        LOGGER.warning(f\"Link bulunamadı in row: {row}\")\n",
    "        return pd.DataFrame()\n",
    "    link = a_tag.get('href')\n",
    "    if link.startswith(PDF_PREFIX):\n",
    "        return pd.DataFrame()\n",
    "    tbody = _tbody_from_link(client, link)\n",
    "    if not tbody:\n",
    "        LOGGER.warning(f\"tbody bulunamadı: {link}\")\n",
    "        return pd.DataFrame()\n",
    "    stocks = []\n",
    "    for table_row in tbody.find_all('tr'):\n",
    "        cols = [c.get_text(strip=True) for c in table_row.find_all('td')]\n",
    "        if len(cols) < 8:\n",
    "            continue\n",
    "        tx_date = cols[1]\n",
    "        ticker = cols[3]\n",
    "        asset_name = cols[4]\n",
    "        asset_type = cols[5]\n",
    "        order_type = cols[6]\n",
    "        tx_amount = cols[7]\n",
    "        if asset_type != 'Stock' and ticker.strip() in ('--', ''):\n",
    "            continue\n",
    "        stocks.append([tx_date, date_received, last, first, order_type, ticker, asset_name, tx_amount])\n",
    "    if stocks:\n",
    "        return pd.DataFrame(stocks, columns=REPORT_COL_NAMES)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "def main(first_name: str = '', last_name: str = '') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    İsteğe bağlı olarak belirli bir senatörün (first_name, last_name) verilerini ya da tüm senatörlerin\n",
    "    verilerini çekip bir DataFrame oluşturur.\n",
    "    \"\"\"\n",
    "    LOGGER.info(\"Client başlatılıyor.\")\n",
    "    client = requests.Session()\n",
    "    client.get = add_rate_limit(client.get)\n",
    "    client.post = add_rate_limit(client.post)\n",
    "    \n",
    "    reports = senator_reports(client, first_name, last_name)\n",
    "    all_txs_list = []\n",
    "    for i, row in enumerate(reports):\n",
    "        if i % 10 == 0:\n",
    "            total = sum(df.shape[0] for df in all_txs_list)\n",
    "            LOGGER.info(f\"Report #{i} işleniyor. Şimdiye kadar {total} işlem çekildi.\")\n",
    "        df = txs_for_report(client, row)\n",
    "        if not df.empty:\n",
    "            all_txs_list.append(df)\n",
    "    if all_txs_list:\n",
    "        all_txs = pd.concat(all_txs_list, ignore_index=True)\n",
    "    else:\n",
    "        all_txs = pd.DataFrame()\n",
    "    return all_txs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-27 17:28:17,332 INFO] Client başlatılıyor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-27 17:28:22,689 INFO] Getting rows starting at offset 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Tüm senatörler için filtrelemek istemiyorsanız parametreleri boş bırakın.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     senator_txs \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotebooks\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[14], line 192\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(first_name, last_name)\u001b[0m\n\u001b[1;32m    189\u001b[0m client\u001b[38;5;241m.\u001b[39mget \u001b[38;5;241m=\u001b[39m add_rate_limit(client\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m    190\u001b[0m client\u001b[38;5;241m.\u001b[39mpost \u001b[38;5;241m=\u001b[39m add_rate_limit(client\u001b[38;5;241m.\u001b[39mpost)\n\u001b[0;32m--> 192\u001b[0m reports \u001b[38;5;241m=\u001b[39m \u001b[43msenator_reports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m all_txs_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reports):\n",
      "Cell \u001b[0;32mIn[14], line 111\u001b[0m, in \u001b[0;36msenator_reports\u001b[0;34m(client, first_name, last_name)\u001b[0m\n\u001b[1;32m    109\u001b[0m token \u001b[38;5;241m=\u001b[39m _csrf(client)\n\u001b[1;32m    110\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 111\u001b[0m reports \u001b[38;5;241m=\u001b[39m \u001b[43mreports_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m all_reports: List[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m reports:\n",
      "Cell \u001b[0;32mIn[14], line 99\u001b[0m, in \u001b[0;36mreports_api\u001b[0;34m(client, offset, token, first_name, last_name)\u001b[0m\n\u001b[1;32m     84\u001b[0m login_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(offset),\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(BATCH_SIZE),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsrfmiddlewaretoken\u001b[39m\u001b[38;5;124m'\u001b[39m: token\n\u001b[1;32m     97\u001b[0m }\n\u001b[1;32m     98\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting rows starting at offset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREPORTS_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogin_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReferer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEARCH_PAGE_URL\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m, in \u001b[0;36madd_rate_limit.<locals>.with_rate_limit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_rate_limit\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 46\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(RATE_LIMIT_SECS)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Tüm senatörler için filtrelemek istemiyorsanız parametreleri boş bırakın.\n",
    "    senator_txs = main(first_name='', last_name='')\n",
    "    output_dir = 'notebooks'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = f'{output_dir}/senators.csv'\n",
    "    senator_txs.to_csv(output_file, index=False)\n",
    "    LOGGER.info(f\"Veriler '{output_file}' dosyasına CSV formatında kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "374ca84d-9e60-47dc-8e2d-f5322536b20c",
       "rows": [],
       "shape": {
        "columns": 0,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senator_txs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-27 17:39:14,748 INFO] Client başlatılıyor.\n",
      "[2025-03-27 17:39:19,535 INFO] Getting rows starting at offset 0\n",
      "[2025-03-27 17:39:21,904 INFO] Getting rows starting at offset 100\n",
      "[2025-03-27 17:39:24,348 INFO] Getting rows starting at offset 200\n",
      "[2025-03-27 17:39:26,702 INFO] Getting rows starting at offset 300\n",
      "[2025-03-27 17:39:29,056 INFO] Getting rows starting at offset 400\n",
      "[2025-03-27 17:39:31,413 INFO] Getting rows starting at offset 500\n",
      "[2025-03-27 17:39:33,771 INFO] Getting rows starting at offset 600\n",
      "[2025-03-27 17:39:36,123 INFO] Getting rows starting at offset 700\n",
      "[2025-03-27 17:39:38,476 INFO] Toplam 674 rapor çekildi.\n",
      "[2025-03-27 17:39:38,477 INFO] Report #0 işleniyor. Şimdiye kadar 0 işlem çekildi.\n",
      "[2025-03-27 17:40:04,404 INFO] Report #10 işleniyor. Şimdiye kadar 55 işlem çekildi.\n",
      "[2025-03-27 17:40:28,975 INFO] Report #20 işleniyor. Şimdiye kadar 130 işlem çekildi.\n",
      "[2025-03-27 17:40:54,912 INFO] Report #30 işleniyor. Şimdiye kadar 171 işlem çekildi.\n",
      "[2025-03-27 17:41:18,235 INFO] Report #40 işleniyor. Şimdiye kadar 213 işlem çekildi.\n",
      "[2025-03-27 17:41:40,826 INFO] Report #50 işleniyor. Şimdiye kadar 228 işlem çekildi.\n",
      "[2025-03-27 17:42:05,298 INFO] Report #60 işleniyor. Şimdiye kadar 269 işlem çekildi.\n",
      "[2025-03-27 17:42:25,974 INFO] Report #70 işleniyor. Şimdiye kadar 303 işlem çekildi.\n",
      "[2025-03-27 17:42:48,563 INFO] Report #80 işleniyor. Şimdiye kadar 325 işlem çekildi.\n",
      "[2025-03-27 17:43:14,768 INFO] Report #90 işleniyor. Şimdiye kadar 340 işlem çekildi.\n",
      "[2025-03-27 17:43:35,786 INFO] Report #100 işleniyor. Şimdiye kadar 366 işlem çekildi.\n",
      "[2025-03-27 17:43:58,851 INFO] Report #110 işleniyor. Şimdiye kadar 496 işlem çekildi.\n",
      "[2025-03-27 17:44:17,938 INFO] Report #120 işleniyor. Şimdiye kadar 591 işlem çekildi.\n",
      "[2025-03-27 17:44:43,281 INFO] Report #130 işleniyor. Şimdiye kadar 620 işlem çekildi.\n",
      "[2025-03-27 17:45:04,194 INFO] Report #140 işleniyor. Şimdiye kadar 652 işlem çekildi.\n",
      "[2025-03-27 17:45:27,742 INFO] Report #150 işleniyor. Şimdiye kadar 766 işlem çekildi.\n",
      "[2025-03-27 17:45:55,156 INFO] Report #160 işleniyor. Şimdiye kadar 853 işlem çekildi.\n",
      "[2025-03-27 17:46:20,467 INFO] Report #170 işleniyor. Şimdiye kadar 889 işlem çekildi.\n",
      "[2025-03-27 17:46:43,141 INFO] Report #180 işleniyor. Şimdiye kadar 919 işlem çekildi.\n",
      "[2025-03-27 17:47:17,332 INFO] Report #190 işleniyor. Şimdiye kadar 1120 işlem çekildi.\n",
      "[2025-03-27 17:47:41,146 INFO] Report #200 işleniyor. Şimdiye kadar 1274 işlem çekildi.\n",
      "[2025-03-27 17:48:04,519 INFO] Report #210 işleniyor. Şimdiye kadar 1307 işlem çekildi.\n",
      "[2025-03-27 17:48:24,805 INFO] Report #220 işleniyor. Şimdiye kadar 1386 işlem çekildi.\n",
      "[2025-03-27 17:48:45,754 INFO] Report #230 işleniyor. Şimdiye kadar 1448 işlem çekildi.\n",
      "[2025-03-27 17:49:11,472 INFO] Report #240 işleniyor. Şimdiye kadar 1631 işlem çekildi.\n",
      "[2025-03-27 17:49:30,237 INFO] Report #250 işleniyor. Şimdiye kadar 1682 işlem çekildi.\n",
      "[2025-03-27 17:49:48,837 INFO] Report #260 işleniyor. Şimdiye kadar 1729 işlem çekildi.\n",
      "[2025-03-27 17:50:09,702 INFO] Report #270 işleniyor. Şimdiye kadar 1781 işlem çekildi.\n",
      "[2025-03-27 17:50:30,274 INFO] Report #280 işleniyor. Şimdiye kadar 1816 işlem çekildi.\n",
      "[2025-03-27 17:50:49,331 INFO] Report #290 işleniyor. Şimdiye kadar 1907 işlem çekildi.\n",
      "[2025-03-27 17:51:10,817 INFO] Report #300 işleniyor. Şimdiye kadar 2037 işlem çekildi.\n",
      "[2025-03-27 17:51:29,940 INFO] Report #310 işleniyor. Şimdiye kadar 2078 işlem çekildi.\n",
      "[2025-03-27 17:51:54,149 INFO] Report #320 işleniyor. Şimdiye kadar 2122 işlem çekildi.\n",
      "[2025-03-27 17:52:16,365 INFO] Report #330 işleniyor. Şimdiye kadar 2169 işlem çekildi.\n",
      "[2025-03-27 17:52:36,377 INFO] Report #340 işleniyor. Şimdiye kadar 2216 işlem çekildi.\n",
      "[2025-03-27 17:52:57,946 INFO] Report #350 işleniyor. Şimdiye kadar 2231 işlem çekildi.\n",
      "[2025-03-27 17:53:18,940 INFO] Report #360 işleniyor. Şimdiye kadar 2248 işlem çekildi.\n",
      "[2025-03-27 17:53:44,844 INFO] Report #370 işleniyor. Şimdiye kadar 2326 işlem çekildi.\n",
      "[2025-03-27 17:54:03,997 INFO] Report #380 işleniyor. Şimdiye kadar 2427 işlem çekildi.\n",
      "[2025-03-27 17:54:23,753 INFO] Report #390 işleniyor. Şimdiye kadar 2548 işlem çekildi.\n",
      "[2025-03-27 17:54:44,624 INFO] Report #400 işleniyor. Şimdiye kadar 2583 işlem çekildi.\n",
      "[2025-03-27 17:55:03,791 INFO] Report #410 işleniyor. Şimdiye kadar 2626 işlem çekildi.\n",
      "[2025-03-27 17:55:26,121 INFO] Report #420 işleniyor. Şimdiye kadar 2654 işlem çekildi.\n",
      "[2025-03-27 17:55:43,131 INFO] Report #430 işleniyor. Şimdiye kadar 2694 işlem çekildi.\n",
      "[2025-03-27 17:56:02,553 INFO] Report #440 işleniyor. Şimdiye kadar 2711 işlem çekildi.\n",
      "[2025-03-27 17:56:24,451 INFO] Report #450 işleniyor. Şimdiye kadar 2747 işlem çekildi.\n",
      "[2025-03-27 17:56:57,160 INFO] Report #460 işleniyor. Şimdiye kadar 2907 işlem çekildi.\n",
      "[2025-03-27 17:57:16,127 INFO] Report #470 işleniyor. Şimdiye kadar 2930 işlem çekildi.\n",
      "[2025-03-27 17:57:38,936 INFO] Report #480 işleniyor. Şimdiye kadar 2947 işlem çekildi.\n",
      "[2025-03-27 17:58:04,739 INFO] Report #490 işleniyor. Şimdiye kadar 3133 işlem çekildi.\n",
      "[2025-03-27 17:58:33,104 INFO] Report #500 işleniyor. Şimdiye kadar 3414 işlem çekildi.\n",
      "[2025-03-27 17:58:56,760 INFO] Report #510 işleniyor. Şimdiye kadar 3439 işlem çekildi.\n",
      "[2025-03-27 17:59:16,829 INFO] Report #520 işleniyor. Şimdiye kadar 3447 işlem çekildi.\n",
      "[2025-03-27 17:59:41,521 INFO] Report #530 işleniyor. Şimdiye kadar 3507 işlem çekildi.\n",
      "[2025-03-27 18:00:02,529 INFO] Report #540 işleniyor. Şimdiye kadar 3535 işlem çekildi.\n",
      "[2025-03-27 18:00:24,002 INFO] Report #550 işleniyor. Şimdiye kadar 3571 işlem çekildi.\n",
      "[2025-03-27 18:00:50,630 INFO] Report #560 işleniyor. Şimdiye kadar 3672 işlem çekildi.\n",
      "[2025-03-27 18:01:10,600 INFO] Report #570 işleniyor. Şimdiye kadar 3680 işlem çekildi.\n",
      "[2025-03-27 18:01:32,416 INFO] Report #580 işleniyor. Şimdiye kadar 3723 işlem çekildi.\n",
      "[2025-03-27 18:01:55,146 INFO] Report #590 işleniyor. Şimdiye kadar 3793 işlem çekildi.\n",
      "[2025-03-27 18:02:18,183 INFO] Report #600 işleniyor. Şimdiye kadar 3844 işlem çekildi.\n",
      "[2025-03-27 18:02:39,502 INFO] Report #610 işleniyor. Şimdiye kadar 3895 işlem çekildi.\n",
      "[2025-03-27 18:03:05,517 INFO] Report #620 işleniyor. Şimdiye kadar 3996 işlem çekildi.\n",
      "[2025-03-27 18:03:24,731 INFO] Report #630 işleniyor. Şimdiye kadar 4069 işlem çekildi.\n",
      "[2025-03-27 18:03:43,450 INFO] Report #640 işleniyor. Şimdiye kadar 4110 işlem çekildi.\n",
      "[2025-03-27 18:04:38,695 INFO] Report #650 işleniyor. Şimdiye kadar 4344 işlem çekildi.\n",
      "[2025-03-27 18:05:04,937 INFO] Report #660 işleniyor. Şimdiye kadar 4701 işlem çekildi.\n",
      "[2025-03-27 18:05:29,428 INFO] Report #670 işleniyor. Şimdiye kadar 4836 işlem çekildi.\n",
      "[2025-03-27 18:05:39,187 INFO] Veriler 'notebooks/senators_2020_2025.csv' dosyasına CSV formatında kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "from typing import List, Optional, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL tanımlamaları\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "LANDING_PAGE_FAIL = 'Failed to fetch filings landing page'\n",
    "\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',\n",
    "    'file_date',\n",
    "    'last_name',\n",
    "    'first_name',\n",
    "    'order_type',\n",
    "    'ticker',\n",
    "    'asset_name',\n",
    "    'tx_amount'\n",
    "]\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "\n",
    "\n",
    "def add_rate_limit(func):\n",
    "    \"\"\"Decorator: Her çağrıdan önce belirli bir süre bekler.\"\"\"\n",
    "    def with_rate_limit(*args, **kwargs):\n",
    "        time.sleep(RATE_LIMIT_SECS)\n",
    "        return func(*args, **kwargs)\n",
    "    return with_rate_limit\n",
    "\n",
    "\n",
    "def _csrf(client: requests.Session) -> str:\n",
    "    \"\"\"\n",
    "    Oturum için CSRF token'ını alır.\n",
    "    Landing page’i ziyaret edip, form üzerinden token'ı çekip gönderir.\n",
    "    \"\"\"\n",
    "    landing_page_response = client.get(LANDING_PAGE_URL)\n",
    "    if landing_page_response.url != LANDING_PAGE_URL:\n",
    "        raise Exception(LANDING_PAGE_FAIL)\n",
    "\n",
    "    landing_page = BeautifulSoup(landing_page_response.text, 'lxml')\n",
    "    csrf_tag = landing_page.find(attrs={'name': 'csrfmiddlewaretoken'})\n",
    "    if not csrf_tag:\n",
    "        raise ValueError(\"CSRF token bulunamadı.\")\n",
    "    form_csrf = csrf_tag.get('value')\n",
    "    form_payload = {\n",
    "        'csrfmiddlewaretoken': form_csrf,\n",
    "        'prohibition_agreement': '1'\n",
    "    }\n",
    "    client.post(LANDING_PAGE_URL, data=form_payload, headers={'Referer': LANDING_PAGE_URL})\n",
    "\n",
    "    csrftoken = client.cookies.get('csrftoken', client.cookies.get('csrf'))\n",
    "    if not csrftoken:\n",
    "        raise ValueError(\"CSRF token cookie'leri bulunamadı.\")\n",
    "    return csrftoken\n",
    "\n",
    "\n",
    "def reports_api(client: requests.Session, offset: int, token: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Periodic transaction reports API'sini sorgular.\n",
    "    Sadece son 5 yıla ait verileri çeker.\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    # Son 5 yıl\n",
    "    start_date = now - timedelta(days=5 * 365)\n",
    "    login_data = {\n",
    "        'start': str(offset),\n",
    "        'length': str(BATCH_SIZE),\n",
    "        'report_types': '[11]',\n",
    "        'filer_types': '[]',\n",
    "        'submitted_start_date': start_date.strftime('%m/%d/%Y 00:00:00'),\n",
    "        'submitted_end_date': now.strftime('%m/%d/%Y %H:%M:%S'),\n",
    "        'candidate_state': '',\n",
    "        'senator_state': '',\n",
    "        'office_id': '',\n",
    "        'first_name': '',\n",
    "        'last_name': '',\n",
    "        'csrfmiddlewaretoken': token\n",
    "    }\n",
    "    LOGGER.info(f'Getting rows starting at offset {offset}')\n",
    "    response = client.post(REPORTS_URL, data=login_data, headers={'Referer': SEARCH_PAGE_URL})\n",
    "    response.raise_for_status()\n",
    "    return response.json()['data']\n",
    "\n",
    "\n",
    "def senator_reports(client: requests.Session) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tüm rapor sonuçlarını döner.\n",
    "    Sadece son 5 yıla ait raporlar çekilir.\n",
    "    \"\"\"\n",
    "    token = _csrf(client)\n",
    "    idx = 0\n",
    "    reports = reports_api(client, idx, token)\n",
    "    all_reports: List[List[str]] = []\n",
    "    while reports:\n",
    "        all_reports.extend(reports)\n",
    "        idx += BATCH_SIZE\n",
    "        reports = reports_api(client, idx, token)\n",
    "    LOGGER.info(f\"Toplam {len(all_reports)} rapor çekildi.\")\n",
    "    return all_reports\n",
    "\n",
    "\n",
    "def _tbody_from_link(client: requests.Session, link: str) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Belirtilen linkteki sayfadan tbody elementini döner.\n",
    "    Eğer bulunamazsa None döner.\n",
    "    \"\"\"\n",
    "    report_url = f'{ROOT}{link}'\n",
    "    report_response = client.get(report_url)\n",
    "    if report_response.url == LANDING_PAGE_URL:\n",
    "        LOGGER.info(\"Oturum süresi doldu. CSRF token yenileniyor.\")\n",
    "        _csrf(client)\n",
    "        report_response = client.get(report_url)\n",
    "    report = BeautifulSoup(report_response.text, 'lxml')\n",
    "    tbodies = report.find_all('tbody')\n",
    "    return tbodies[0] if tbodies else None\n",
    "\n",
    "\n",
    "def txs_for_report(client: requests.Session, row: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bir rapor satırından hisse alım-satım işlemleri DataFrame'i oluşturur.\n",
    "    \"\"\"\n",
    "    if len(row) < 5:\n",
    "        LOGGER.warning(f\"Yetersiz sütun içeren satır: {row}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    first, last, _, link_html, date_received = row[:5]\n",
    "    soup_link = BeautifulSoup(link_html, 'lxml')\n",
    "    a_tag = soup_link.find('a')\n",
    "    if not a_tag:\n",
    "        LOGGER.warning(f\"Link bulunamadı in row: {row}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    link = a_tag.get('href')\n",
    "    if link.startswith(PDF_PREFIX):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    tbody = _tbody_from_link(client, link)\n",
    "    if not tbody:\n",
    "        LOGGER.warning(f\"tbody bulunamadı: {link}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    stocks = []\n",
    "    for table_row in tbody.find_all('tr'):\n",
    "        cols = [c.get_text(strip=True) for c in table_row.find_all('td')]\n",
    "        if len(cols) < 8:\n",
    "            continue\n",
    "        tx_date = cols[1]\n",
    "        ticker = cols[3]\n",
    "        asset_name = cols[4]\n",
    "        asset_type = cols[5]\n",
    "        order_type = cols[6]\n",
    "        tx_amount = cols[7]\n",
    "        if asset_type != 'Stock' and ticker.strip() in ('--', ''):\n",
    "            continue\n",
    "        stocks.append([tx_date, date_received, last, first, order_type, ticker, asset_name, tx_amount])\n",
    "    \n",
    "    if stocks:\n",
    "        return pd.DataFrame(stocks, columns=REPORT_COL_NAMES)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def main() -> pd.DataFrame:\n",
    "    LOGGER.info(\"Client başlatılıyor.\")\n",
    "    client = requests.Session()\n",
    "    client.get = add_rate_limit(client.get)\n",
    "    client.post = add_rate_limit(client.post)\n",
    "    \n",
    "    reports = senator_reports(client)\n",
    "    all_txs_list = []\n",
    "    for i, row in enumerate(reports):\n",
    "        if i % 10 == 0:\n",
    "            total = sum(df.shape[0] for df in all_txs_list)\n",
    "            LOGGER.info(f\"Report #{i} işleniyor. Şimdiye kadar {total} işlem çekildi.\")\n",
    "        df = txs_for_report(client, row)\n",
    "        if not df.empty:\n",
    "            all_txs_list.append(df)\n",
    "    \n",
    "    if all_txs_list:\n",
    "        all_txs = pd.concat(all_txs_list, ignore_index=True)\n",
    "    else:\n",
    "        all_txs = pd.DataFrame()\n",
    "    \n",
    "    return all_txs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    senator_txs = main()\n",
    "    \n",
    "    # Çıktının kaydedileceği klasörü oluştur\n",
    "    output_dir = 'notebooks'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Son 5 yılın başlangıç ve bitiş yılını belirle\n",
    "    now = datetime.now()\n",
    "    five_years_ago = now - timedelta(days=5 * 365)\n",
    "    output_file = f'{output_dir}/senators_{five_years_ago.year}_{now.year}.csv'\n",
    "    \n",
    "    # DataFrame'i CSV dosyasına kaydet (index sütunu eklenmez)\n",
    "    senator_txs.to_csv(output_file, index=False)\n",
    "    LOGGER.info(f\"Veriler '{output_file}' dosyasına CSV formatında kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
